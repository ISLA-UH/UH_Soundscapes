{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b9827e2e-92c7-4c19-a2c9-f70e11ebd68b",
   "metadata": {},
   "source": [
    "# Data Fusion Tutorial: Standardizing, Streamlining, and Merging Data from Multiple Datasets\n",
    "\n",
    "This tutorial goes over the basics of how to read, standardize, extract metadata from, and merge data from multiple datasets. The resulting merged dataset file can be used, after some additional processing, to train and test machine learning models.\n",
    "\n",
    "We'll be merging data from 4 publicly available acoustic datasets: \n",
    "- Aggregated Smartphone Timeseries of Rocket-generated Acoustics (ASTRA)\n",
    "- Smartphone High-explosives Audio Recordings Dataset (SHAReD)\n",
    "- Hypersonic signals from the OSIRIS-REx capsule reenty (OREX UH)\n",
    "- Environmental sound recordings from ESC-50, downsampled to 800 Hz\n",
    "\n",
    "Pickle (PKL) files of each of these datasets can be downloaded from the __[Soundscapes Archive](https://www.higp.hawaii.edu/archive/isla/UH_Soundscapes/)__. Single-recording subsets of each are also included with this repository so that the tutorial can be run without first downloaded the full datasets, if desired.\n",
    "\n",
    "For information on each of the datasets, see the references listed in `README.md`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cd4c7f7",
   "metadata": {},
   "source": [
    "## Section 0: Prerequisites and Imports\n",
    "The following cell includes the imports necessary to run this example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68c63af8-7640-43a7-8e3c-f636870e4f4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from typing import Dict, Tuple\n",
    "\n",
    "import pandas as pd\n",
    "pd.options.mode.copy_on_write = True\n",
    "\n",
    "import uh_soundscapes.standard_labels as stl\n",
    "import uh_soundscapes.dataset_standardization as ds_std"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59505de5-3a00-4310-9ec0-54e3eca99309",
   "metadata": {},
   "source": [
    "## Section 1: Loading the Datasets\n",
    "\n",
    "In the following cell, we'll define the paths to the dataset. By default, this notebook will use the single-recording tutorial files included in the code repository. If you've downloaded the full datasets and would like to use them, change the file path variables `IMPORT_DIRECTORY`, `ASTRA_FILENAME`, `SHARED_FILENAME`, `ESC50_FILENAME`, and `OREX_FILENAME` to the directory and file names of the datasets on your machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3263ebaa-131d-48e2-9db0-6f309772d578",
   "metadata": {},
   "outputs": [],
   "source": [
    "CURRENT_DIRECTORY = os.getcwd()\n",
    "IMPORT_DIRECTORY = CURRENT_DIRECTORY\n",
    "ASTRA_FILENAME = \"ASTRA_tutorial.pkl\"\n",
    "SHARED_FILENAME = \"SHAReD_tutorial.pkl\"\n",
    "OREX_FILENAME = \"OREX_tutorial.pkl\"\n",
    "ESC50_FILENAME = \"ESC50_tutorial_800Hz.pkl\"\n",
    "\n",
    "# ASTRA_FILENAME = \"<insert path to ASTRA.pkl on your device here>\"\n",
    "# SHARED_FILENAME = \"<insert path to SHAReD.pkl on your device here>\"\n",
    "# OREX_FILENAME = \"<insert path to OREX_UH_800Hz.pkl on your device here>\"\n",
    "# ESC50_FILENAME = \"<insert path to ESC50_800Hz.pkl on your device here>\"\n",
    "# IMPORT_DIRECTORY = \"<insert path to directory containing the above files on your device here>\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3008422",
   "metadata": {},
   "source": [
    "We'll also define a few other filenames for exporting various files. If you'd like to export files to a different directory, change the value of `EXPORT_DIRECTORY` to the path to the directory you'd like to save files to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1a23342",
   "metadata": {},
   "outputs": [],
   "source": [
    "EXPORT_DIRECTORY = CURRENT_DIRECTORY\n",
    "\n",
    "ASTRA_STANDARDIZED_FILENAME = \"ASTRA_standardized.pkl\"\n",
    "ASTRA_EVENT_MD_FILENAME = \"ASTRA_event_metadata.csv\"\n",
    "ASTRA_STATION_MD_FILENAME = \"ASTRA_station_metadata.csv\"\n",
    "\n",
    "SHARED_STANDARDIZED_FILENAME = \"SHAReD_standardized.pkl\"\n",
    "SHARED_EVENT_MD_FILENAME = \"SHAReD_event_metadata.csv\"\n",
    "SHARED_STATION_MD_FILENAME = \"SHAReD_station_metadata.csv\"\n",
    "\n",
    "OREX_STANDARDIZED_FILENAME = \"OREX_standardized.pkl\"\n",
    "OREX_STATION_MD_FILENAME = \"OREX_station_metadata.csv\"\n",
    "\n",
    "ESC50_STANDARDIZED_FILENAME = \"ESC50_800Hz_standardized.pkl\"\n",
    "ESC50_EVENT_MD_FILENAME = \"ESC50_800Hz_event_metadata.csv\"\n",
    "\n",
    "MERGED_FILENAME = \"merged_standardized_dataset.pkl\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52b11af7-fe5f-435d-8dc6-3bbed89622a4",
   "metadata": {},
   "source": [
    "We'll can now read each dataset using the pandas module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61a34be7-1910-40b5-bf30-d5dd776e0b5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "astra_ds = pd.read_pickle(os.path.join(IMPORT_DIRECTORY, ASTRA_FILENAME))\n",
    "shared_ds = pd.read_pickle(os.path.join(IMPORT_DIRECTORY, SHARED_FILENAME))\n",
    "orex_ds = pd.read_pickle(os.path.join(IMPORT_DIRECTORY, OREX_FILENAME))\n",
    "esc50_ds = pd.read_pickle(os.path.join(IMPORT_DIRECTORY, ESC50_FILENAME))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15efa308-5fad-4f07-8357-527b22ffd0aa",
   "metadata": {},
   "source": [
    "Each row of each of the pandas DataFrames contains all data and metadata from a single recording. The column names and values in each DataFrame vary, however.\n",
    "\n",
    "To keep track of all the column names, we can use the labels classes in `uh_soundscapes.standard_labels`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8229ac12-cdde-49f3-af48-68f03321cc88",
   "metadata": {},
   "outputs": [],
   "source": [
    "STL = stl.StandardLabels()\n",
    "SL = stl.SHAReDLabels()\n",
    "AL = stl.ASTRALabels()\n",
    "EL = stl.ESC50Labels()\n",
    "OL = stl.OREXLabels()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "009b7333-68c3-4fbc-840d-9f0506a3f803",
   "metadata": {},
   "source": [
    "Just like in the individual dataset tutorial notebooks, we'll use these classes to easily access different fields in the dataset. In this tutorial, however, we'll also use them to extract the metadata and standardize the data column values and names so that the datasets can be merged.\n",
    "\n",
    "In the following sections, we'll go through how this is done for each of the four datasets. For a script version of this process, see `uh_soundscapes.dataset_standardization`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de271d96",
   "metadata": {},
   "source": [
    "## Section 2: Streamlining and Standardizing ASTRA\n",
    "\n",
    "In this section, we'll be streamlining and standardizing the ASTRA dataset so that it can be merged with the other datasets. Since the primary motivation for this is to be able to use the merged dataset for machine learning applications, we want to reduce the size of the file as much as possible. For reference, we'll print the size of the input file below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6b11ce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "astra_input_size_bytes = os.path.getsize(os.path.join(IMPORT_DIRECTORY, ASTRA_FILENAME))\n",
    "print(f\"Input ASTRA file size: {astra_input_size_bytes / 1e6:.4f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8bee40d",
   "metadata": {},
   "source": [
    "The ASTRA dataset includes a number of fields containing metadata on the rocket launches the recordings are from as well as the recording stations themselves. Since there are multiple recordings from each launch and by each station, we can reduce the size of the dataset significantly by extracting this metadata and storing it in separate files.\n",
    "\n",
    "In the next cell, we'll do this using the `uh_soundscapes.dataset_standardization` function `compile_metadata`. For details on how this function works, see documentation in `uh_soundscapes.dataset_standardization`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a08e2ff0-dd86-4bf2-95c5-9defa099049e",
   "metadata": {},
   "outputs": [],
   "source": [
    "astra_event_metadata = ds_std.compile_metadata(astra_ds, AL.launch_id, AL.event_metadata)\n",
    "astra_station_metadata = ds_std.compile_metadata(astra_ds, AL.station_id, AL.station_metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc0ea3a0",
   "metadata": {},
   "source": [
    "There are a handful of fields in `StandardLabels` that are not in the ASTRA dataset. We want these fields in the final, merged dataset, so we'll add them to the ASTRA data. Three of these fields (`STL.station_alt`, `STL.data_source`, and `STL.station_network`) are common to all recordings in ASTRA, so we'll add them now.\n",
    "\n",
    "<b>Note</b>: We are using the float value -9999.9 as a placeholder for an unknown surface altitude. Since the station latitude and longitude are included in the dataset and the stations are all on or near the surface, we can get the true altitudes from topography data and add them in later, if desired."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f23c485d",
   "metadata": {},
   "outputs": [],
   "source": [
    "UNKN_SURF_ALT = -9999.9  # placeholder for unknown surface altitude\n",
    "n_rows_astra = len(astra_ds)\n",
    "astra_ds[STL.station_alt] = [UNKN_SURF_ALT] * n_rows_astra  # ASTRA stations are all surface stations\n",
    "astra_ds[STL.data_source] = [\"ASTRA\"] * n_rows_astra # all data is from the ASTRA dataset\n",
    "astra_ds[STL.station_network] = [\"FLORIDA\"] * n_rows_astra  # all data was recorded on the Florida network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2afba58c",
   "metadata": {},
   "source": [
    "In addition, the audio recordings in ASTRA are 5-10 minutes in duration. The full-duration recordings are useful and interesting for many applications, but we recommend starting with only the high-amplitude main launch signature for machine learning applications. This will also reduce the size of the dataset significantly.\n",
    "\n",
    "We also want to keep samples of the pre-launch ambient noise if possible. Including these samples in training mitigates against station-bias, ensuring the model is not being trained to classify signals by their recording station rather than their origin.\n",
    "\n",
    "To do this, we'll make two copies of the dataset and select the rocket and noise samples, respectively, from each. To see how this is done in detail, look through the documentation and comments of the `uh_soundscapes.dataset_standardization` functions `select_astra_rocket_samples()` and `select_astra_noise_samples()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "012dbc12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a copies of the raw dataframe to select samples from\n",
    "rocket_astra_ds = astra_ds.copy()\n",
    "noise_astra_ds = astra_ds.copy()\n",
    "# select 5 second rocket samples centered on the peak aligned time of arrival\n",
    "rocket_astra_ds = ds_std.select_astra_rocket_samples(rocket_astra_ds)\n",
    "# select < 50 second noise samples ending at least 60 seconds before the start-aligned time of arrival\n",
    "noise_astra_ds = ds_std.select_astra_noise_samples(noise_astra_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f5a8bc1",
   "metadata": {},
   "source": [
    "We'll now fill in the other `StandardLabels` columns we want in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f5fe777",
   "metadata": {},
   "outputs": [],
   "source": [
    "rocket_astra_ds[STL.source_alt] = [UNKN_SURF_ALT] * len(rocket_astra_ds)  # ASTRA launches are all on the surface\n",
    "rocket_astra_ds[STL.ml_label] = [\"rocket\"] * len(rocket_astra_ds)  # suggested label for ML applications\n",
    "\n",
    "noise_astra_ds[STL.ml_label] = [\"noise\"] * len(noise_astra_ds)  # suggested label for ML applications"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "206151e5",
   "metadata": {},
   "source": [
    "Next, we'll rename the column names to the `StandardLabels` equivalents using the `uh_soundscapes.standard_labels` function `standardize_df_columns()` and the standarization dictionary included in `ASTRALabels`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3dce830",
   "metadata": {},
   "outputs": [],
   "source": [
    "rocket_astra_ds = stl.standardize_df_columns(dataset=rocket_astra_ds, label_map=AL.standardize_dict)\n",
    "noise_astra_ds = stl.standardize_df_columns(dataset=noise_astra_ds, label_map=AL.standardize_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8143e926",
   "metadata": {},
   "source": [
    "We'll reset the `noise_astra_ds` source location and time values to NaNs (since the source of the sounds in these samples are not from the launch), then check to see if any of the standard columns are missing from either `rocket_astra_ds` and/or `noise_astra_ds`. For any missing standard columns, we'll add the column and fill it with NaNs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b61301ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset source location and time columns to NaN\n",
    "noise_astra_ds[STL.source_lat] = [np.nan] * len(noise_astra_ds)\n",
    "noise_astra_ds[STL.source_lon] = [np.nan] * len(noise_astra_ds)\n",
    "noise_astra_ds[STL.source_alt] = [np.nan] * len(noise_astra_ds)\n",
    "noise_astra_ds[STL.source_epoch_s] = [np.nan] * len(noise_astra_ds)\n",
    "# fill in any other missing standard columns with NaNs\n",
    "for col in STL.standard_labels:\n",
    "    if col not in noise_astra_ds.columns:\n",
    "        noise_astra_ds[col] = [np.nan] * len(noise_astra_ds)\n",
    "    if col not in rocket_astra_ds.columns:\n",
    "        rocket_astra_ds[col] = [np.nan] * len(rocket_astra_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77ff7e04",
   "metadata": {},
   "source": [
    "Finally, we'll reduce each of the two DataFrames to only the standard columns, then concatenate them, creating a single ASTRA dataset once again, this time with only the data we need to train and test machine learning models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f72d3136",
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep only standard columns\n",
    "rocket_astra_ds = rocket_astra_ds[STL.standard_labels]\n",
    "noise_astra_ds = noise_astra_ds[STL.standard_labels]\n",
    "# concatenate rocket and noise dataframes\n",
    "astra_standardized_ds = pd.concat([rocket_astra_ds, noise_astra_ds], ignore_index=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5435eda2",
   "metadata": {},
   "source": [
    "Let's export the standardized dataset and the metadata files, then check how much we've reduced the size of the ASTRA data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e21ba7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "astra_standardized_ds.to_pickle(os.path.join(EXPORT_DIRECTORY, ASTRA_STANDARDIZED_FILENAME))\n",
    "astra_event_metadata.to_csv(os.path.join(EXPORT_DIRECTORY, ASTRA_EVENT_MD_FILENAME), index=True)\n",
    "astra_station_metadata.to_csv(os.path.join(EXPORT_DIRECTORY, ASTRA_STATION_MD_FILENAME), index=True)\n",
    "astra_output_size_bytes = os.path.getsize(os.path.join(EXPORT_DIRECTORY, ASTRA_STANDARDIZED_FILENAME))\n",
    "print(f\"ASTRA file size before standarization: {astra_input_size_bytes / 1e6:.4f} MB\")\n",
    "print(f\"ASTRA file size after standarization: {astra_output_size_bytes / 1e6:.4f} MB\")\n",
    "print(f\"ASTRA file size REDUCED by: {(astra_input_size_bytes - astra_output_size_bytes) / astra_input_size_bytes * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a57707c",
   "metadata": {},
   "source": [
    "Before moving on to the next dataset, we'll take a look at the contents of the metadata files we just exported.\n",
    "\n",
    "The first file contains the metadata in ASTRA that is associated with the launch event, indexed by the unique launch ID strings. The metadata fields are:\n",
    "- 'launch_id': the ID string of the launch\n",
    "- 'launch_pad_latitude': the latitude of the launch pad in degrees\n",
    "- 'launch_pad_longitude': the longitude of the launch pad in degrees\n",
    "- 'reported_launch_epoch_s': the reported launch time in epoch seconds\n",
    "- 'rocket_type': the type of rocket launched (make and model name)\n",
    "- 'rocket_model_number': the model number of the rocket launched\n",
    "- 'n_solid_rocket_boosters': the number of solid rocket boosters used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acc7cbe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "astra_event_metadata.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d242bd5",
   "metadata": {},
   "source": [
    "The second file contains the metadata in ASTRA that is associated with the recording smartphone station, indexed by the unique station ID strings. The metadata fields are:\n",
    "- 'station_id': the ID string of the launch\n",
    "- 'station_make': the make of the smartphone\n",
    "- 'station_model_number': the model number of the smartphone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96e841e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "astra_station_metadata.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea80bac6",
   "metadata": {},
   "source": [
    "## Section 3: Streamlining and Standardizing SHAReD\n",
    "\n",
    "In this section, we'll be streamlining and standardizing the SHAReD dataset so that it can be merged with the other datasets. Like with ASTRA, we want to reduce the size of the file as much as possible. For reference, we'll print the size of the input file below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa2602e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "shared_input_size_bytes = os.path.getsize(os.path.join(IMPORT_DIRECTORY, SHARED_FILENAME))\n",
    "print(f\"Input SHARED file size: {shared_input_size_bytes / 1e6:.4f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "821d6d6a",
   "metadata": {},
   "source": [
    "Like ASTRA, the SHAReD dataset includes a number of fields containing metadata on the explosions the recordings are from as well as the recording stations themselves. Since there are multiple recordings from each explosion and by each station, we can reduce the size of the dataset significantly by extracting this metadata and storing it in separate files.\n",
    "\n",
    "Before we do this, however, we'll make one quick change. If you've completed the SHAReD tutorial notebook, you'll remember that each explosion event in the dataset is associated with a non-unique `event_name` string as well as a unique `event_id_number` integer. To align with the other datasets, we'll combine these two fields so that the `event_name` strings are unique to each event."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f18fb1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# change NNSS event names from \"NNSS\" to \"NNSS_<event_id_number>\" to make them unique\n",
    "for idx in shared_ds.index:\n",
    "    if shared_ds[SL.event_name][idx] == \"NNSS\":\n",
    "        shared_ds.at[idx, SL.event_name] = f\"NNSS_{shared_ds[SL.event_id_number][idx]}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d53ba2f",
   "metadata": {},
   "source": [
    "Now that the `event_name` values are unique to individual events, we'll extract the metadata. In the next cell, we'll do this using the `compile_metadata` function just like we did with ASTRA in the previous section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc2358e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "shared_event_metadata = ds_std.compile_metadata(shared_ds, SL.event_id_number, SL.event_metadata)\n",
    "shared_station_metadata = ds_std.compile_metadata(shared_ds, SL.smartphone_id, SL.station_metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5231e847",
   "metadata": {},
   "source": [
    "At this point, we'll add the missing standard columns that have the same values for both the 'explosion' and 'ambient' signals in SHAReD: `data_source`, `station_alt`, and `station_network`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2483da15",
   "metadata": {},
   "outputs": [],
   "source": [
    "shared_ds[STL.data_source] = [\"SHAReD\"] * len(shared_ds) # all data is from the SHAReD dataset\n",
    "shared_ds[STL.station_alt] = [UNKN_SURF_ALT] * len(shared_ds)  # placeholder for unknown surface altitude\n",
    "shared_ds[STL.station_network] = [x.split(\"_\")[0] for x in shared_ds[SL.event_name]] # network is first part of event name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed09e30a",
   "metadata": {},
   "source": [
    "To significantly reduce the size of SHAReD, we'll remove the all the columns unrelated to the audio, location, and time data. We'll do this separately for the 'explosion' and 'ambient' samples of the audio recordings already separated and labeled in SHAReD, creating two DataFrames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e1bab6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# columns to keep for the explosion DataFrame\n",
    "explosion_columns = [SL.event_name, SL.smartphone_id, SL.microphone_data,\n",
    "                     SL.microphone_time_s, SL.microphone_sample_rate_hz,\n",
    "                     SL.external_location_latitude, SL.external_location_longitude,\n",
    "                     SL.source_latitude, SL.source_longitude, SL.explosion_detonation_time,\n",
    "                     STL.data_source, STL.station_alt, STL.station_network]\n",
    "# columns to keep for the ambient DataFrame\n",
    "ambient_columns = [SL.event_name, SL.smartphone_id, SL.ambient_microphone_time_s,\n",
    "                   SL.ambient_microphone_data, SL.microphone_sample_rate_hz,\n",
    "                   SL.external_location_latitude, SL.external_location_longitude,\n",
    "                   SL.source_latitude, SL.source_longitude, \n",
    "                   STL.data_source, STL.station_alt, STL.station_network]\n",
    "# create separate DataFrames for explosion and ambient data\n",
    "explosion_df = shared_ds[explosion_columns]\n",
    "ambient_df = shared_ds[ambient_columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "088b7395",
   "metadata": {},
   "source": [
    "Now that the 'explosion' and 'ambient' signals are separated, we'll rename the columns to their standard names, just like we did with ASTRA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b9be05f",
   "metadata": {},
   "outputs": [],
   "source": [
    "explosion_df = stl.standardize_df_columns(dataset=explosion_df, label_map=SL.standardize_dict)\n",
    "ambient_df = stl.standardize_df_columns(dataset=ambient_df, label_map=SL.standardize_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b986c0ac",
   "metadata": {},
   "source": [
    "The next standard column we'll add is the column containing the epoch second of the first point in the waveform, after which we can eliminate the full time arrays associated with each audio waveform, further reducing the size of the dataset. \n",
    "\n",
    "<b>Note:</b> If the time array is required later, it can always be reconstructed from this single time value, the sample rate, and the length of the waveform array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d33d271",
   "metadata": {},
   "outputs": [],
   "source": [
    "explosion_df[STL.t0_epoch_s] = [t[0] for t in explosion_df[SL.microphone_time_s]]\n",
    "ambient_df[STL.t0_epoch_s] = [t[0] for t in ambient_df[SL.ambient_microphone_time_s]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b585389d",
   "metadata": {},
   "source": [
    "We'll also add the `ml_label` and `source_alt` columns from `StandardLabels`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e8798be",
   "metadata": {},
   "outputs": [],
   "source": [
    "explosion_df[STL.ml_label] = [\"explosion\"] * len(explosion_df)\n",
    "ambient_df[STL.ml_label] = [\"silence\"] * len(ambient_df)\n",
    "\n",
    "explosion_df[STL.source_alt] = [UNKN_SURF_ALT] * len(explosion_df)  # explosions are all on the surface\n",
    "ambient_df[STL.source_alt] = [np.nan] * len(ambient_df)  # SHAReD ambient data has no identified source"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "263b349f",
   "metadata": {},
   "source": [
    "Now, we'll check to see if the standard columns are all present in both DataFrames. For any missing columns, we'll add them and fill with NaNs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa583f60",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in STL.standard_labels:\n",
    "    if col not in explosion_df.columns:\n",
    "        explosion_df[col] = [np.nan] * len(explosion_df)\n",
    "    if col not in ambient_df.columns:\n",
    "        ambient_df[col] = [np.nan] * len(ambient_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1be08d5",
   "metadata": {},
   "source": [
    "Next, we'll ensure the source location and time columns are filled with NaNs for the ambient data, which has no identified source."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaca13bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset source location and time columns to NaN\n",
    "ambient_df[STL.source_lat] = [np.nan] * len(ambient_df)\n",
    "ambient_df[STL.source_lon] = [np.nan] * len(ambient_df)\n",
    "ambient_df[STL.source_alt] = [np.nan] * len(ambient_df)\n",
    "ambient_df[STL.source_epoch_s] = [np.nan] * len(ambient_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaf2797c",
   "metadata": {},
   "source": [
    "Finally, we'll reduce each of the two DataFrames to only the standard columns, then concatenate them, creating a single SHAReD dataset once again, this time with only the data we need to train and test machine learning models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad072b74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep only standard columns\n",
    "ambient_df = ambient_df[STL.standard_labels]\n",
    "explosion_df = explosion_df[STL.standard_labels]\n",
    "# concatenate explosion and ambient dataframes\n",
    "shared_standardized_ds = pd.concat([explosion_df, ambient_df], ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ddd2580",
   "metadata": {},
   "source": [
    "Let's export the standardized dataset and the metadata files, then check how much we've reduced the size of the SHAReD data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7db4b5ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "shared_standardized_ds.to_pickle(os.path.join(EXPORT_DIRECTORY, SHARED_STANDARDIZED_FILENAME))\n",
    "shared_event_metadata.to_csv(os.path.join(EXPORT_DIRECTORY, SHARED_EVENT_MD_FILENAME), index=True)\n",
    "shared_station_metadata.to_csv(os.path.join(EXPORT_DIRECTORY, SHARED_STATION_MD_FILENAME), index=True)\n",
    "shared_output_size_bytes = os.path.getsize(os.path.join(EXPORT_DIRECTORY, SHARED_STANDARDIZED_FILENAME))\n",
    "print(f\"SHARED file size before standarization: {shared_input_size_bytes / 1e6:.4f} MB\")\n",
    "print(f\"SHARED file size after standarization: {shared_output_size_bytes / 1e6:.4f} MB\")\n",
    "print(f\"SHARED file size REDUCED by: {(shared_input_size_bytes - shared_output_size_bytes) / shared_input_size_bytes * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99850435",
   "metadata": {},
   "source": [
    "Before moving on to the next dataset, we'll take a look at the contents of the metadata files we just exported.\n",
    "\n",
    "The first file contains the metadata in SHAReD that is associated with the explosion event, indexed by the unique event name strings. The metadata fields are:\n",
    "- 'event_name': the ID string of the explosion event\n",
    "- 'training_validation_test': an integer unique to the explosion event\n",
    "- 'source_yield_kg': the source yield of the event in equivalent kilograms of TNT\n",
    "- 'effective_yield_category': the effective yield category the event belongs to\n",
    "- 'source_latitude': the latitude of the explosion site in degrees\n",
    "- 'source_longitude': the longitude of the explosion site in degrees\n",
    "- 'explosion_detonation_time': the detonation time of the explosion in epoch seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcb45979",
   "metadata": {},
   "outputs": [],
   "source": [
    "shared_event_metadata.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5bcd5ca",
   "metadata": {},
   "source": [
    "The second file contains the metadata in SHAReD that is associated with the recording smartphone station, indexed by the unique station ID strings. The metadata fields are:\n",
    "- 'smartphone_id': the ID string of the smartphone\n",
    "- 'station_model': the model of the smartphone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a55ac583",
   "metadata": {},
   "outputs": [],
   "source": [
    "shared_station_metadata.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc529ac0-d5a3-4b50-a72f-ff54b2ab6d65",
   "metadata": {},
   "source": [
    "## Section 4: Streamlining and Standardizing OREX Data\n",
    "\n",
    "In this section, we'll be streamlining and standardizing the OREX_UH dataset so that it can be merged with the other datasets. Like with previous datasets, we want to reduce the size of the file as much as possible. For reference, we'll print the size of the input file below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c974346",
   "metadata": {},
   "outputs": [],
   "source": [
    "orex_input_size_bytes = os.path.getsize(os.path.join(IMPORT_DIRECTORY, OREX_FILENAME))\n",
    "print(f\"Input OREX file size: {orex_input_size_bytes / 1e6:.4f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07fde320",
   "metadata": {},
   "source": [
    "We'll start by adding some ground truth information to the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d32a233",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_orex_signals = len(orex_ds)\n",
    "orex_ds[OL.audio_fs] = [800.] * n_orex_signals  # all OREX signals were recorded at 800 Hz\n",
    "orex_ds[OL.event_id] = [\"OREX\"] * n_orex_signals  # all OREX signals are from the OSIRIS-REx reentry"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17194def",
   "metadata": {},
   "source": [
    "We'll also extract the station model and station network from the station label strings using the functions and mapping dictionary defined in the next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b510a21",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_station_network(station_label_string):\n",
    "    return station_label_string.split(\" \")[0]\n",
    "\n",
    "def get_station_model_key(station_label_string):\n",
    "    return station_label_string.split(\" \")[-1].split(\"-\")[0]\n",
    "\n",
    "station_model_mapping = {\n",
    "    \"S08\": {'make': \"Samsung\", 'model': \"Galaxy S8\"},\n",
    "    \"S10\": {'make': \"Samsung\", 'model': \"Galaxy S10\"},\n",
    "    \"S21\": {'make': \"Samsung\", 'model': \"Galaxy S21\"},\n",
    "    \"S22\": {'make': \"Samsung\", 'model': \"Galaxy S22\"},\n",
    "    \"S23\": {'make': \"Samsung\", 'model': \"Galaxy S23\"},\n",
    "    \"A53\": {'make': \"Samsung\", 'model': \"Galaxy A53\"},\n",
    "    \"T06\": {'make': \"Samsung\", 'model': \"Galaxy Tab 6\"},\n",
    "}\n",
    "\n",
    "orex_ds[OL.station_network] = [get_station_network(sls) for sls in orex_ds[OL.station_label]]\n",
    "\n",
    "station_model_keys = [get_station_model_key(sls) for sls in orex_ds[OL.station_label]]\n",
    "\n",
    "orex_ds[OL.station_make] = [station_model_mapping[key]['make'] for key in station_model_keys]\n",
    "orex_ds[OL.station_model] = [station_model_mapping[key]['model'] for key in station_model_keys]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c93af0a",
   "metadata": {},
   "source": [
    "Unlike ASTRA and SHAReD, there's very little data in the OREX_UH file that is unnecessary. We can still extract station metadata, but as all the recordings in the OREX dataset are from the OSIRIS-REx reentry, there's no event metadata to extract. For information about the event, see the references listed in the README file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aa5ed17-8d7a-4a67-8c38-4ed6b9938453",
   "metadata": {},
   "outputs": [],
   "source": [
    "orex_station_metadata = ds_std.compile_metadata(orex_ds, OL.station_id, OL.station_metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ad7cc6e",
   "metadata": {},
   "source": [
    "We'll now standardize the column names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aa98a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "orex_standardized_ds = stl.standardize_df_columns(dataset=orex_ds, label_map=OL.standardize_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edba4959",
   "metadata": {},
   "source": [
    "Next, we can add the rest of the standard columns with known values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1743494",
   "metadata": {},
   "outputs": [],
   "source": [
    "orex_standardized_ds[STL.data_source] = [\"UH_OREX\"] * n_orex_signals # all data is from the UH OREX dataset\n",
    "orex_standardized_ds[STL.station_alt] = [UNKN_SURF_ALT] * n_orex_signals  # placeholder for unknown surface altitude\n",
    "orex_standardized_ds[STL.ml_label] = [\"hypersonic\"] * n_orex_signals  # suggested label for ML applications\n",
    "orex_standardized_ds[STL.t0_epoch_s] = [time[0] for time in orex_standardized_ds[OL.audio_epoch_s]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c234636",
   "metadata": {},
   "source": [
    "Finally, we'll fill any missing standard columns with NaNs and reduce the dataset to only the standard columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "103ad4d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill in any missing standard columns with NaNs\n",
    "for col in STL.standard_labels:\n",
    "    if col not in orex_standardized_ds.columns:\n",
    "        orex_standardized_ds[col] = [np.nan] * n_orex_signals\n",
    "# keep only the standard columns\n",
    "orex_standardized_ds = orex_standardized_ds[STL.standard_labels]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42be8f9a",
   "metadata": {},
   "source": [
    "Let's export the standardized dataset and the station metadata file, then check how much we've reduced the size of the OREX data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "463f1785",
   "metadata": {},
   "outputs": [],
   "source": [
    "orex_standardized_ds.to_pickle(os.path.join(EXPORT_DIRECTORY, OREX_STANDARDIZED_FILENAME))\n",
    "orex_station_metadata.to_csv(os.path.join(EXPORT_DIRECTORY, OREX_STATION_MD_FILENAME), index=True)\n",
    "orex_output_size_bytes = os.path.getsize(os.path.join(EXPORT_DIRECTORY, OREX_STANDARDIZED_FILENAME))\n",
    "print(f\"OREX file size before standarization: {orex_input_size_bytes / 1e6:.4f} MB\")\n",
    "print(f\"OREX file size after standarization: {orex_output_size_bytes / 1e6:.4f} MB\")\n",
    "print(f\"OREX file size REDUCED by: {(orex_input_size_bytes - orex_output_size_bytes) / orex_input_size_bytes * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de476832",
   "metadata": {},
   "source": [
    "Before moving on, we'll take a look at the station metadata file. The metadata fields are:\n",
    "- 'station_ids': the ID string of the smartphone\n",
    "- 'station_labels': the station label string of the smartphone\n",
    "- 'station_make': the make of the smartphone\n",
    "- 'station_model_number': the model of the smartphone\n",
    "- 'deployment_network': the name of the network the smartphone was deployed on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d35d2a5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "orex_station_metadata.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79abb863",
   "metadata": {},
   "source": [
    "## Section 5: Streamlining and Standardizing ESC-50 Data\n",
    "\n",
    "In this section, we'll be streamlining and standardizing the ESC-50 dataset so that it can be merged with the other datasets. Like with previous datasets, we want to reduce the size of the file as much as possible. However, ESC-50 is already very streamlined and this will not be possible. Instead, we'll aim to increase the size as little as possible. For reference, we'll print the size of the input file below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46157c48",
   "metadata": {},
   "outputs": [],
   "source": [
    "esc50_input_size_bytes = os.path.getsize(os.path.join(IMPORT_DIRECTORY, ESC50_FILENAME))\n",
    "print(f\"Input ESC-50 file size: {esc50_input_size_bytes / 1e6:.4f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d0674da",
   "metadata": {},
   "source": [
    "We'll start by adding the data source column to the DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86b357ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_esc50_signals = len(esc50_ds)\n",
    "esc50_ds[STL.data_source] = [\"ESC-50\"] * n_esc50_signals # all data is from the ESC-50 dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cca85e4",
   "metadata": {},
   "source": [
    "We can then compile event metadata for the dataset. ESC-50 contains no information on the recording stations, so there is no station metadata to complile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a14a0cdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "esc50_event_metadata = ds_std.compile_metadata(esc50_ds, EL.clip_id, EL.event_metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69d0aadf",
   "metadata": {},
   "source": [
    "We'll now standardize the column names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1960c5c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "esc50_standardized_ds = stl.standardize_df_columns(dataset=esc50_ds, label_map=EL.standardize_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de123146",
   "metadata": {},
   "source": [
    "Finally, we'll fill any missing standard columns with NaNs and reduce the dataset to only the standard columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7003026",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill in any missing standard columns with NaNs\n",
    "for col in STL.standard_labels:\n",
    "    if col not in esc50_standardized_ds.columns:\n",
    "        esc50_standardized_ds[col] = [np.nan] * n_esc50_signals\n",
    "# keep only the standard columns\n",
    "esc50_standardized_ds = esc50_standardized_ds[STL.standard_labels]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "490d2324",
   "metadata": {},
   "source": [
    "Let's export the standardized dataset and the station metadata file, then check how much we've reduced the size of the ESC-50 data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc8de780",
   "metadata": {},
   "outputs": [],
   "source": [
    "esc50_standardized_ds.to_pickle(os.path.join(EXPORT_DIRECTORY, ESC50_STANDARDIZED_FILENAME))\n",
    "esc50_event_metadata.to_csv(os.path.join(EXPORT_DIRECTORY, ESC50_EVENT_MD_FILENAME), index=True)\n",
    "esc50_output_size_bytes = os.path.getsize(os.path.join(EXPORT_DIRECTORY, ESC50_STANDARDIZED_FILENAME))\n",
    "print(f\"ESC-50 file size before standarization: {esc50_input_size_bytes / 1e6:.4f} MB\")\n",
    "print(f\"ESC-50 file size after standarization: {esc50_output_size_bytes / 1e6:.4f} MB\")\n",
    "print(f\"ESC-50 file size INCREASED by: {(esc50_output_size_bytes - esc50_input_size_bytes) / esc50_input_size_bytes * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e068c75",
   "metadata": {},
   "source": [
    "Before moving on, we'll take a look at the station metadata file. The metadata fields are:\n",
    "- 'clip_id': the ID string of the Freesound clip the audio is sampled from\n",
    "- 'true_class': the ESC-50 class of the sample\n",
    "- 'inferred_class': the class predicted by YAMNet when run on the sample (after upsampling to 16kHz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c8c8721",
   "metadata": {},
   "outputs": [],
   "source": [
    "esc50_event_metadata.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be3d21d0",
   "metadata": {},
   "source": [
    "## Section 6: Data Fusion\n",
    "\n",
    "Now that all the datasets are standardized, they can be easily merged into one dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20c515e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets_to_merge = [astra_standardized_ds, shared_standardized_ds, orex_standardized_ds, esc50_standardized_ds]\n",
    "merged_ds = pd.concat(datasets_to_merge, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1921b5c2",
   "metadata": {},
   "source": [
    "Before exporting the merged dataset, we'll print out summaries of each of the included datasets and the resulting merged dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9d8d5d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset in datasets_to_merge:\n",
    "    ds_std.summarize_dataset(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c6c0a69",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"MERGED DATASET\")\n",
    "ds_std.summarize_dataset(merged_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bce5135",
   "metadata": {},
   "source": [
    "Finally, we'll export the merged dataset and check to see how big the file is compared to the original datasets, combined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8abf9dfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_ds.to_pickle(os.path.join(EXPORT_DIRECTORY, MERGED_FILENAME))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6fc1485",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_output_size_bytes = os.path.getsize(os.path.join(EXPORT_DIRECTORY, MERGED_FILENAME))\n",
    "initial_combined_size_bytes = (astra_input_size_bytes + shared_input_size_bytes + orex_input_size_bytes + esc50_input_size_bytes)\n",
    "print(f\"Combined input file size: {initial_combined_size_bytes / 1e6:.4f} MB\")\n",
    "print(f\"Merged file size: {final_output_size_bytes / 1e6:.4f} MB\")\n",
    "print(f\"Total file size REDUCED by: {(initial_combined_size_bytes - final_output_size_bytes) / initial_combined_size_bytes * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8185411",
   "metadata": {},
   "source": [
    "This concludes the tutorial on standardizing and merging the datasets for use with machine learning applications."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
