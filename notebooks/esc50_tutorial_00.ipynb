{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b9827e2e-92c7-4c19-a2c9-f70e11ebd68b",
   "metadata": {},
   "source": [
    "# Environmental Sound Classification Dataset Tutorial: Loading, Filtering, Plotting, and Saving ESC-50 Data\n",
    "\n",
    "This tutorial goes over the basics of how to read, filter, and plot data from a curated PKL file containing the audio waveforms and associated metadata from ESC-50, an open-access dataset of environmental sound recordings.\n",
    "\n",
    "ESC-50 was released by Karol Piczak in 2015. For information on ESC-50, see its __[original paper](https://doi.org/10.1145/2733373.2806390)__, __[webpage on Papers with Code](https://paperswithcode.com/dataset/esc-50)__, or __[GitHub repository.](https://github.com/karolpiczak/ESC-50)__.\n",
    "\n",
    "PKL files containing a single ESC-50 recording downsampled to 800Hz, 8kHz, and 16kHz, respectively, are included with this tutorial. The full ESC-50 dataset can be downloaded as a collection of .wav files at 44.1kHz sampling frequency from __[GitHub](https://github.com/karolpiczak/ESC-50)__ or consolidated PKL files downsampled to 800Hz, 8kHz, or 16kHz sampling frequency from __[Soundscapes Archive](https://www.higp.hawaii.edu/archive/isla/UH_Soundscapes/ESC50/)__. Conversion to numpy arrays and downsampling was performed using the tensorflow function audio.decode_wav() and the tensorflow-io function audio.resample().\n",
    "\n",
    "For examples of how to use ESC-50 data in machine learning applications, see __[Takazawa, et al. (2025)](https://doi.org/10.3390/s24206688)__ and __[Popenhagen, et al. (2025)](https://doi.org/10.3390/signals6030041)__\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cd4c7f7",
   "metadata": {},
   "source": [
    "## Section 0: Prerequisites and Imports\n",
    "The following cell includes the imports necessary to run this tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68c63af8-7640-43a7-8e3c-f636870e4f4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime, timezone, timedelta\n",
    "from scipy import signal\n",
    "from quantum_inferno.plot_templates.plot_templates_examples import plot_wf_mesh_vert_example\n",
    "from quantum_inferno.cwt_atoms import cwt_chirp_from_sig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59505de5-3a00-4310-9ec0-54e3eca99309",
   "metadata": {},
   "source": [
    "## Section 1: Loading the Dataset\n",
    "\n",
    "In the following cell, we'll define the path to the dataset. By default, this path will point to the single-recording subsets of ESC-50 included with this tutorial.\n",
    "\n",
    "After you've completed the tutorial with this file, feel free to download the full ESC-50 dataset PKL files and use the last three lines of the cell to change the `PATH_TO_PKL_800`, `PATH_TO_PKL_8k`, and `PATH_TO_PKL_16k` variables to point to the 'ESC50_800Hz.pkl', 'ESC50_8kHz.pkl', and 'ESC50_16kHz.pkl' file locations on your device."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3263ebaa-131d-48e2-9db0-6f309772d578",
   "metadata": {},
   "outputs": [],
   "source": [
    "TUTORIAL_PICKLE_FILE_NAME_800 = \"ESC50_tutorial_800Hz.pkl\"\n",
    "TUTORIAL_PICKLE_FILE_NAME_8k = \"ESC50_tutorial_8kHz.pkl\"\n",
    "TUTORIAL_PICKLE_FILE_NAME_16k = \"ESC50_tutorial_16kHz.pkl\"\n",
    "\n",
    "CURRENT_DIRECTORY = os.getcwd()\n",
    "\n",
    "PATH_TO_TUTORIAL_PKL_800 = os.path.join(CURRENT_DIRECTORY, TUTORIAL_PICKLE_FILE_NAME_800)\n",
    "PATH_TO_TUTORIAL_PKL_8k = os.path.join(CURRENT_DIRECTORY, TUTORIAL_PICKLE_FILE_NAME_8k)\n",
    "PATH_TO_TUTORIAL_PKL_16k = os.path.join(CURRENT_DIRECTORY, TUTORIAL_PICKLE_FILE_NAME_16k)\n",
    "PATH_TO_PKL_800 = PATH_TO_TUTORIAL_PKL_800\n",
    "PATH_TO_PKL_8k = PATH_TO_TUTORIAL_PKL_8k\n",
    "PATH_TO_PKL_16k = PATH_TO_TUTORIAL_PKL_16k\n",
    "\n",
    "# PATH_TO_PKL_800 = \"<insert path to ESC50_800Hz.pkl on your device here>\"\n",
    "#PATH_TO_PKL_8k = \"<insert path to ESC50_8kHz.pkl on your device here>\"\n",
    "# PATH_TO_PKL_16k = \"<insert path to ESC50_16kHz.pkl on your device here>\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52b11af7-fe5f-435d-8dc6-3bbed89622a4",
   "metadata": {},
   "source": [
    "Once we have the locations of the files, we can read the data using the pandas module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61a34be7-1910-40b5-bf30-d5dd776e0b5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "esc50_ds_16k = pd.read_pickle(PATH_TO_PKL_16k)\n",
    "esc50_ds_8k = pd.read_pickle(PATH_TO_PKL_8k)\n",
    "esc50_ds_800 = pd.read_pickle(PATH_TO_PKL_800)\n",
    "esc50_ds_16k.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b50194e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "esc50_ds_8k.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d628bae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "esc50_ds_800.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15efa308-5fad-4f07-8357-527b22ffd0aa",
   "metadata": {},
   "source": [
    "The structure of the three pandas DataFrames (800Hz, 8kHz, and 16kHz) is identical. Each row contains the identification number of the Freesound audio file the 5 s long audio clip was taken from, the downsampled waveform, the sampling rate the waveform was downsampled to (800Hz, 8kHz, or 16kHz), the target (an integer value assigned to the ESC-50 class associated with the clip), the name of the ESC-50 class ('true' class), and the name of the class predicted by Google's YAMNet audio classification model using the mean score over the record. Different YAMNET classes at different sample rates are obtained by using the median score. The original ESC class should be used as the more accurate annotation.\n",
    "\n",
    "To keep track of all the column names, we'll define the 'ESC50Labels' class, and then initiate an instance of it in the next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53b5767e-add7-4000-841d-6e41e0db6f52",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ESC50Labels:\n",
    "    \"\"\"\n",
    "    A class containing the column names used in the ESC-50 pickle files.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "            self,\n",
    "            clip_id: str = \"clip_id\",\n",
    "            audio_data: str = \"waveform\",\n",
    "            audio_fs: str = \"fs\",\n",
    "            esc50_target: str = \"target\",\n",
    "            esc50_true_class: str = \"true_class\",\n",
    "            yamnet_predicted_class: str = \"inferred_class\",\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Defaults should be left in place for compatibility with the ESC-50 pickle files.\n",
    "        :param clip_id: the ID string of the Freesound clip the audio was taken from, e.g. \"freesound123456\"\n",
    "        :param audio_data: a numpy array containing the raw audio waveform amplitudes\n",
    "        :param audio_fs: the sampling frequency of the audio waveform in Hz, e.g. 800 or 16000\n",
    "        :param esc50_target: the target class number of the ESC-50 class, e.g. 37 for \"clock_alarm\"\n",
    "        :param esc50_true_class: the name of the true ESC-50 class, e.g. \"clock_alarm\"\n",
    "        :param yamnet_predicted_class: the name of the top class predicted by YAMNet, e.g. \"Tools\"\n",
    "        \"\"\"\n",
    "        self.clip_id = clip_id\n",
    "        self.audio_data = audio_data\n",
    "        self.audio_fs = audio_fs\n",
    "        self.esc50_target = esc50_target\n",
    "        self.esc50_true_class = esc50_true_class\n",
    "        self.yamnet_predicted_class = yamnet_predicted_class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5d6b573",
   "metadata": {},
   "source": [
    "<b>Tip:</b> When writing your own scripts, you can instead import the class from the UH-Soundscapes package with the following line:\n",
    "\n",
    "```python\n",
    "from soundscapes.standard_labels import ESC50Labels\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8229ac12-cdde-49f3-af48-68f03321cc88",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_labels = ESC50Labels()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "009b7333-68c3-4fbc-840d-9f0506a3f803",
   "metadata": {},
   "source": [
    "With the labels now easily accessible, we'll print out some metadata about the recording(s) in the files..\n",
    "\n",
    "Notice how the desired fields are accessed using the column names stored in `ds_labels`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a08e2ff0-dd86-4bf2-95c5-9defa099049e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for esc50_ds in [esc50_ds_16k, esc50_ds_8k, esc50_ds_800]:\n",
    "    fs = int(esc50_ds[ds_labels.audio_fs][esc50_ds.index[0]])\n",
    "    print(f\"\\nESC-50 dataset at {fs}Hz:\")\n",
    "    # get some details about the dataset and print them out\n",
    "    n_signals = len(esc50_ds)\n",
    "    n_clips = len(np.unique(esc50_ds[ds_labels.clip_id]))\n",
    "    classes, counts = np.unique(esc50_ds[ds_labels.esc50_true_class], return_counts=True)\n",
    "    print(f\"\\tThis dataset contains {n_signals} 5 s long samples from {n_clips} different Freesound audio clips assigned to\")\n",
    "    print(f\"\\t{len(classes)} unique classes.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2c3087a-1474-40b2-bb5b-606ad12ed57f",
   "metadata": {},
   "source": [
    "We can also loop through each Freesound ID in the dataset and look at the classes of the samples taken from them. The samples are the same in each dataset, so we'll just look at the 16kHz dataset for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dba43ec-207a-44ff-9c4f-9979324a0a42",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\nFreesound clips and the sample taken from them:\")\n",
    "clips = np.unique(esc50_ds_16k[ds_labels.clip_id])\n",
    "for clip_id in clips:\n",
    "    clip_ds = esc50_ds_16k[esc50_ds_16k[ds_labels.clip_id] == clip_id]\n",
    "    classes, counts = np.unique(clip_ds[ds_labels.esc50_true_class], return_counts=True)\n",
    "    class_summary = f\"\\t{clip_id}: \"\n",
    "    for class_name, class_count in zip(classes, counts):\n",
    "        class_summary += f\"'{class_name}': {class_count} sample(s),\"\n",
    "    print(class_summary[:-1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc529ac0-d5a3-4b50-a72f-ff54b2ab6d65",
   "metadata": {},
   "source": [
    "## Section 2: Filtering the Dataset\n",
    "\n",
    "The files can be filtered easily using any of the included metadata fields. For example, you could select a subset of the 800Hz dataset containing only samples classified by YAMNet as \"Heartbeat\" using this line of code:\n",
    "\n",
    "```python\n",
    "heartbeat_df_800 = esc50_ds_800[esc50_ds_800[ds_labels.yamnet_inferred_class] == \"Heartbeat\"]\n",
    "```\n",
    "\n",
    "For this tutorial, we'll select a subset of the dataset containing only the data associated with the clip ID 'freesound103050'.\n",
    "\n",
    "By selecting only those rows with \"freesound103050\" in their clip ID field, we create a subset of the dataset containing all available data from our chosen Freesound clip and no data from any other audio clips in the file.\n",
    "\n",
    "In the output of the next three cells, we see that the subsets we've created each have only one row. To confirm that this is correct, we can scroll through the output of the previous cell and to find the line summarizing our example clip: `freesound103050: 'thunderstorm': 1 sample(s)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aa5ed17-8d7a-4a67-8c38-4ed6b9938453",
   "metadata": {},
   "outputs": [],
   "source": [
    "ex_clip_id: str = \"freesound103050\"\n",
    "ex_clip_df_800 = esc50_ds_800[esc50_ds_800[ds_labels.clip_id] == ex_clip_id]\n",
    "ex_clip_df_8k = esc50_ds_8k[esc50_ds_8k[ds_labels.clip_id] == ex_clip_id]\n",
    "ex_clip_df_16k = esc50_ds_16k[esc50_ds_16k[ds_labels.clip_id] == ex_clip_id]\n",
    "ex_clip_df_800"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e21cb47",
   "metadata": {},
   "outputs": [],
   "source": [
    "ex_clip_df_8k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7f52dd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "ex_clip_df_16k"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b01f40dd-c539-49ef-8ddf-dd5592781564",
   "metadata": {},
   "source": [
    "## Section 3: Plotting ESC-50 Data\n",
    "\n",
    "To plot ESC-50 audio data, the time array must be reconstructed from the sample rate and length of the waveform.  We use numpy to do this.\n",
    "\n",
    "In the following cell, we'll define two functions: one to calculate the rolling mean of a waveform and another to visualize the data from a single ESC-50 sample in the time domain as well as the frequency domain. Read through the comments in the functions for a detailed explanation of each step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb94429d-988a-4e2a-a2bd-059b08d36242",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a colorblind-friendly color cycle to use in our plots\n",
    "CBF_COLOR_CYCLE = ['#377eb8', '#ff7f00', '#4daf4a', '#f781bf', '#a65628', '#984ea3', '#999999', '#e41a1c', '#dede00']\n",
    "\n",
    "\n",
    "def rolling_mean(signal: np.ndarray, fs: float, window_length_s: float = 0.01) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Calculate the rolling mean of a signal using a specified window length in seconds.\n",
    "\n",
    "    :param signal: The input signal as a NumPy array\n",
    "    :param window_length_s: The length of the rolling window in seconds\n",
    "    :return: A NumPy array containing the rolling mean of the input signal\n",
    "    \"\"\"\n",
    "    pad: int = int(fs * window_length_s / 2)\n",
    "    window_size: int = int(pad * 2 + 1)\n",
    "    roll_mean = []\n",
    "    for idx_i in range(len(signal)):\n",
    "        if idx_i <= pad:\n",
    "            sig_slice = signal[:window_size]\n",
    "        elif idx_i + pad >= len(signal):\n",
    "            sig_slice = signal[-window_size:]\n",
    "        else:\n",
    "            sig_slice = signal[int(idx_i - pad): int(idx_i + pad + 1)]\n",
    "        if len(sig_slice) != window_size:\n",
    "            raise ValueError(f\"Signal slice length {len(sig_slice)} does not match window size {window_size}.\")\n",
    "        roll_mean.append(np.nanmean(sig_slice))\n",
    "    return np.array(roll_mean)\n",
    "\n",
    "\n",
    "\n",
    "def single_sample_example_plots(esc50_ds, ds_labels, sample_idx, plot_tfr=True, roll_mean=True, fontsize=12, figure_size=(10,7)):\n",
    "    sample_fs = esc50_ds[ds_labels.audio_fs][sample_idx]\n",
    "    sample_waveform = esc50_ds[ds_labels.audio_data][sample_idx]\n",
    "    # We'll demean and normalize the waveform to the range [-1, 1] for cleaner visualization\n",
    "    if roll_mean:\n",
    "        # Calculate the mean of the waveform with a rolling window approximately 0.01 seconds in duration\n",
    "        sample_mean = rolling_mean(sample_waveform, fs=sample_fs, window_length_s=0.01)\n",
    "    else:\n",
    "        # Calculate the mean of the whole waveform\n",
    "        sample_mean = np.nanmean(sample_waveform)\n",
    "    # Demean\n",
    "    sample_waveform = sample_waveform - sample_mean\n",
    "    # Normalize to the range [-1, 1]\n",
    "    sample_waveform = sample_waveform / np.nanmax(np.abs(sample_waveform))\n",
    "\n",
    "    # We'll also extract the true class, the class predicted by YAMNet, and the clip ID of the sample to add to the plot title\n",
    "    sample_esc50_class = esc50_ds[ds_labels.esc50_true_class][sample_idx]\n",
    "    sample_yamnet_class = esc50_ds[ds_labels.yamnet_predicted_class][sample_idx]\n",
    "    sample_clip_id = esc50_ds[ds_labels.clip_id][sample_idx]\n",
    "\n",
    "    # To visualize the data in the time domain, we must reconstruct the time array\n",
    "    sample_time_array = np.arange(len(sample_waveform)) / sample_fs\n",
    "\n",
    "    # To visualize the frequency content of the sample, we'll calculate and plot the Welch \n",
    "    # power spectral density (PSD) of the waveform as well.\n",
    "    nperseg = sample_fs * 0.48  # 0.48 seconds per segment\n",
    "    f, Pxx_den = signal.welch(sample_waveform, sample_fs, nperseg=nperseg)\n",
    "\n",
    "    # Time domain and frequency domain figure set-up\n",
    "    fig, ax = plt.subplots(2, 1, figsize=figure_size)\n",
    "    xlabel = \"Time (s)\"\n",
    "    title = f\"ESC-50 PKL index {sample_idx} audio downsampled to {int(sample_fs)}Hz\"\n",
    "    title += f\"\\nClip ID: {sample_clip_id}, ESC-50 class: {sample_esc50_class}\\nClass predicted by YAMNet\"\n",
    "    if sample_fs < 16000.0:\n",
    "        title += \" after upsampling\"\n",
    "    title += f\": {sample_yamnet_class}\"\n",
    "    # Plot the waveform\n",
    "    ax[0].plot(sample_time_array, sample_waveform, lw=1, color=\"k\")\n",
    "    # Plot the PSD\n",
    "    ax[1].plot(f, Pxx_den, lw=1, color=\"k\")\n",
    "    # Time domain panel settings\n",
    "    ax[0].set(xlim=(sample_time_array[0], sample_time_array[-1]), ylim=(-1.1, 1.1))\n",
    "    ax[0].set_title(title, fontsize=fontsize + 2)\n",
    "    ax[0].set_xlabel(xlabel, fontsize=fontsize)\n",
    "    ax[0].set_ylabel(\"Normalized waveform\", fontsize=fontsize)\n",
    "    # Frequency domain panel settings\n",
    "    ax[1].set(xlim=(0, sample_fs / 2), ylim=(0, np.max(Pxx_den) * 1.05))\n",
    "    ax[1].set_xlabel(\"Frequency (Hz)\", fontsize=fontsize)\n",
    "    ax[1].set_ylabel(\"Power spectral density (PSD)\", fontsize=fontsize)\n",
    "\n",
    "    plt.subplots_adjust()\n",
    "\n",
    "    # If desired, we can also visualize the data in the time-frequency domain. To do this, we calculate and plot the \n",
    "    # continuous wavelet transform (CWT) of the audio data using functions in the quantum_inferno module\n",
    "    if plot_tfr:\n",
    "        tfr_title = f\"CWT and waveform from ESC-50 PKL index {sample_idx} (clip ID {sample_clip_id})\"\n",
    "        _, cwt_bits, time_s, frequency_cwt_hz = cwt_chirp_from_sig(\n",
    "            sig_wf=sample_waveform,\n",
    "            frequency_sample_rate_hz=sample_fs,\n",
    "            band_order_nth=3,\n",
    "        )\n",
    "        _ = plot_wf_mesh_vert_example(\n",
    "            station_id=\"\",\n",
    "            wf_panel_a_sig=sample_waveform,\n",
    "            wf_panel_a_time=sample_time_array,\n",
    "            mesh_time=time_s,\n",
    "            mesh_frequency=frequency_cwt_hz,\n",
    "            mesh_panel_b_tfr=cwt_bits,\n",
    "            figure_title=tfr_title,\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08e9fc15-93da-4c07-8aee-033174c192e8",
   "metadata": {},
   "source": [
    "To generate the time domain plots for our example clip, we'll call our plotting function with `plot_tfr=False`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db4cf5bd-152f-4c2d-ae1e-617538d83cf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "for ex_clip_df in [ex_clip_df_800, ex_clip_df_8k,ex_clip_df_16k]:\n",
    "    single_sample_example_plots(\n",
    "        esc50_ds=ex_clip_df,\n",
    "        ds_labels=ds_labels,\n",
    "        sample_idx=ex_clip_df.index[0],\n",
    "        plot_tfr=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0154b3cb-7e11-4be3-9d77-36a72155a576",
   "metadata": {},
   "source": [
    "If we want to generate the time frequency representations, we can call the same plotting function but with `plot_tfr=True`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6c0838b-4041-401c-90f6-a8b1d46b7113",
   "metadata": {},
   "outputs": [],
   "source": [
    "for ex_clip_df in [ex_clip_df_800, ex_clip_df_8k, ex_clip_df_16k]:\n",
    "    single_sample_example_plots(\n",
    "        esc50_ds=ex_clip_df,\n",
    "        ds_labels=ds_labels,\n",
    "        sample_idx=ex_clip_df.index[0],\n",
    "        plot_tfr=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21c384ee-fdb4-4cea-9b2b-21ba3b3f4950",
   "metadata": {},
   "source": [
    "If we're using PKL files with multiple samples, we can loop through and generate plots for each sample easily, but the cell will take a while to run (3-5s for each sample), so we'll limit it to the first 5 samples in the files. To see more (or different) samples, change the slice indices in the first line of the next cell. For example, to plot the sixth through 10th samples, the line would read:\n",
    "\n",
    "```python\n",
    "indices_to_plot = esc50_ds_800.index[5:10]\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e211f3bf-8312-4999-80c1-cf1929144104",
   "metadata": {},
   "outputs": [],
   "source": [
    "indices_to_plot = esc50_ds_800.index[0:5]\n",
    "for idx in indices_to_plot:\n",
    "    for esc50_ds in [esc50_ds_800, esc50_ds_8k, esc50_ds_16k]:\n",
    "        single_sample_example_plots(\n",
    "            esc50_ds=esc50_ds,\n",
    "            ds_labels=ds_labels,\n",
    "            sample_idx=idx,\n",
    "            plot_tfr=False)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "402695c2-90b9-43ef-a61d-08c6958ad9bd",
   "metadata": {},
   "source": [
    "## Section 4: Saving ESC-50 Data\n",
    "\n",
    "We can also save a subset of the data to a new pickle (.pkl) file. This can be useful in some cases as the full dataset is quite large and may not be needed for all applications.\n",
    "\n",
    "We can save any subset (samples labeled 'dog', samples classified by YAMNet as 'clapping', etc.), but we'll stick with our example subset of data from a single Freesound clip. For this example, we'll save PKL files of our example thunderstorm clip, but this can be modified by simply changing the value of `clip_id_to_save`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1823decc-d09e-4d05-b372-95babb2ccd50",
   "metadata": {},
   "outputs": [],
   "source": [
    "clip_id_to_save = \"freesound103050\"\n",
    "for esc50_ds in [esc50_ds_800, esc50_ds_8k,esc50_ds_16k]:\n",
    "    # Check if the clip id is in the dataset\n",
    "    if clip_id_to_save in np.unique(esc50_ds[ds_labels.clip_id]):\n",
    "        subset_to_save = esc50_ds[esc50_ds[ds_labels.clip_id] == clip_id_to_save]\n",
    "        # Print some details about the subset\n",
    "        n_samples = len(subset_to_save)\n",
    "        classes, class_counts = np.unique(subset_to_save[ds_labels.esc50_true_class], return_counts=True)\n",
    "        print_string = f\"Selected clip: {clip_id_to_save}, ESC-50 classes:\"\n",
    "        for class_name, class_count in zip(classes, class_counts):\n",
    "            print_string += f\" {class_name} ({class_count} sample(s)),\"\n",
    "        print(print_string[:-1])\n",
    "        \n",
    "        # Save the subset DataFrame to a new pickle file\n",
    "        fs = esc50_ds[ds_labels.audio_fs][esc50_ds.index[0]]\n",
    "        if fs < 1000:\n",
    "            output_filename = f\"ESC50_{clip_id_to_save}_{int(fs)}Hz.pkl\"\n",
    "        else:\n",
    "            output_filename = f\"ESC50_{clip_id_to_save}_{int(fs/1000)}kHz.pkl\"\n",
    "        output_path = os.path.join(CURRENT_DIRECTORY, output_filename)\n",
    "        print(f\"Saving ESC-50 data from clip {clip_id_to_save} to: {output_path}\")\n",
    "        subset_to_save.to_pickle(output_path)\n",
    "    else:\n",
    "        print(\"Requested data not found. No file saved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d63f51f-d222-4e5a-bf16-666c9c454428",
   "metadata": {},
   "source": [
    "This concludes the tutorial. For more details on ESC-50, see the references listed at the beginning of the tutorial."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
